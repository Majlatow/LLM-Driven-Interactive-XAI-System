{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, pipeline, TextGenerationPipeline\n",
    "import accelerate\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import json\n",
    "import dice_ml\n",
    "from dice_ml import Dice\n",
    "from dice_ml.utils import helpers\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "import shap\n",
    "import xgboost\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from haystack import component, Document, Pipeline, tracing\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever, InMemoryBM25Retriever\n",
    "from haystack.components.builders import ChatPromptBuilder, PromptBuilder\n",
    "from haystack.components.joiners import BranchJoiner\n",
    "from haystack.components.routers import ConditionalRouter\n",
    "from haystack.components.converters import OutputAdapter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "from haystack.components.generators import HuggingFaceLocalGenerator\n",
    "from haystack.components.generators.chat import HuggingFaceLocalChatGenerator\n",
    "\n",
    "from haystack_experimental.chat_message_stores.in_memory import InMemoryChatMessageStore\n",
    "from haystack_experimental.components.retrievers import ChatMessageRetriever\n",
    "from haystack_experimental.components.writers import ChatMessageWriter\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "import logging\n",
    "from haystack import tracing\n",
    "from haystack.tracing.logging_tracer import LoggingTracer\n",
    "\n",
    "import gradio as gr\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: Installing farm-haystack and haystack-ai in the same Python environment (virtualenv, Colab, or system) causes problems.\n",
    "Installing both packages in the same environment can somehow work or fail in obscure ways. We suggest installing only one of these packages per Python environment. Make sure that you remove both packages if they are installed in the same environment, followed by installing only one of them:\n",
    "\n",
    "pip uninstall -y farm-haystack haystack-ai\n",
    "\n",
    "pip install haystack-ai\n",
    "\n",
    "//pip install git+https://github.com/deepset-ai/haystack.git@main "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preparation\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create PyTorch Datasets\n",
    "class CancerDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = CancerDataset(X_train, y_train)\n",
    "test_dataset = CancerDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define and Train the DNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define and Train the DNN Model\n",
    "\n",
    "class SimpleDNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleDNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = SimpleDNN(input_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        predicted = (outputs >= 0.5).int()\n",
    "        predictions.extend(predicted.numpy())\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generate Counterfactual Explanations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generate Counterfactual Explanations Using DiCE\n",
    "\n",
    "# Prepare data in a pandas DataFrame\n",
    "X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "y_test_df = pd.Series(y_test, name='target')\n",
    "test_df = X_test_df.copy()\n",
    "test_df['target'] = y_test_df\n",
    "\n",
    "# Combine train and test for DiCE\n",
    "X_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "y_train_df = pd.Series(y_train, name='target')\n",
    "train_df = X_train_df.copy()\n",
    "train_df['target'] = y_train_df\n",
    "\n",
    "# Ensure feature_names is a list\n",
    "if isinstance(feature_names, np.ndarray):\n",
    "    feature_names = feature_names.tolist()\n",
    "\n",
    "# Initialize DiCE data object with continuous_features as a list\n",
    "d = dice_ml.Data(\n",
    "    dataframe=train_df,\n",
    "    continuous_features=feature_names,  # Ensure this is a list\n",
    "    outcome_name='target'\n",
    ")\n",
    "\n",
    "# Initialize DiCE model wrapper for PyTorch\n",
    "class PyTorchModelWrapper(nn.Module):\n",
    "    def __init__(self, model, scaler):\n",
    "        super(PyTorchModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If x is a pandas DataFrame or Series, convert it to a NumPy array\n",
    "        if isinstance(x, (pd.DataFrame, pd.Series)):\n",
    "            x = x.values\n",
    "\n",
    "        # Ensure x is a NumPy array\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.detach().numpy()\n",
    "\n",
    "        # Scale the input data\n",
    "        x_scaled = self.scaler.transform(x)\n",
    "        x_tensor = torch.tensor(x_scaled, dtype=torch.float32)\n",
    "\n",
    "        # Pass the scaled data through the model\n",
    "        outputs = self.model(x_tensor)\n",
    "        return outputs\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Use the forward method to get model outputs\n",
    "        outputs = self.forward(x).detach().numpy().squeeze()\n",
    "\n",
    "        # Since the model outputs probabilities for class 1, compute class 0 probabilities\n",
    "        probs_class_1 = outputs\n",
    "        probs_class_0 = 1 - probs_class_1\n",
    "\n",
    "        # Stack probabilities\n",
    "        probs = np.vstack([probs_class_0, probs_class_1]).T\n",
    "        return probs\n",
    "\n",
    "# Instantiate the model wrapper\n",
    "model_wrapper = PyTorchModelWrapper(model, scaler)\n",
    "\n",
    "# Initialize DiCE model with correct backend\n",
    "m = dice_ml.Model(model=model_wrapper, backend='PYT', model_type='classifier')\n",
    "\n",
    "# Initialize DiCE explainer\n",
    "explainer_DiCE = Dice(d, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generate SHAP Explanations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create SHAP explanation, global + local\n",
    "\n",
    "# Convert the PyTorch model to a SHAP-compatible model\n",
    "class ShapModelWrapper:\n",
    "    def __init__(self, model, scaler):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Ensure input is in the right format\n",
    "        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "            X = X.values\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(X_tensor).numpy().squeeze()\n",
    "        return output\n",
    "\n",
    "# Instantiate the SHAP model wrapper\n",
    "shap_model_wrapper = ShapModelWrapper(model, scaler)\n",
    "\n",
    "# Select a subset of the data to use with SHAP to avoid memory issues\n",
    "X_shap = X_test_df\n",
    "\n",
    "# Generate SHAP explainer and values\n",
    "explainer_SHAP = shap.Explainer(shap_model_wrapper.predict, X_train_df)\n",
    "shap_values = explainer_SHAP(X_shap)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_shap, show=False)\n",
    "plt.savefig('shap_summary_plot.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# # SHAP Feature Importance Plot\n",
    "clust = shap.utils.hclust(X, y, linkage=\"single\")\n",
    "plt.figure()\n",
    "shap.plots.bar(shap_values, max_display = 31)\n",
    "plt.savefig('shap_feature_importance_plot.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# SHAP Feature Importance Plot + Clustering\n",
    "clust = shap.utils.hclust(X, y, linkage=\"single\")\n",
    "plt.figure()\n",
    "shap.plots.bar(shap_values, clustering=clust, clustering_cutoff=0.5, max_display = 31)\n",
    "plt.savefig('shap_feature_importance_clustering_plot.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "this_y_pred = (shap_values.values.sum(1) + shap_values[0].base_values) > 0\n",
    "this_misclassified = this_y_pred != y_test_df\n",
    "shap.decision_plot(base_value=shap_values[0].base_values, shap_values=shap_values.values[5], feature_names = feature_names, highlight=this_misclassified[5], alpha=0.5)\n",
    "shap.decision_plot(base_value=shap_values[0].base_values, shap_values=shap_values.values[0:], feature_names = feature_names, highlight=this_misclassified[0:], feature_display_range=slice(None, None, -1), alpha=0.5)\n",
    "shap.decision_plot(base_value=shap_values[0].base_values, shap_values=shap_values.values, feature_names = feature_names, highlight=this_misclassified, feature_display_range=slice(None, None, -1), alpha=0.5, feature_order=\"hclust\")\n",
    "\n",
    "\n",
    "# Store SHAP values in a DataFrame\n",
    "shap_df = pd.DataFrame(shap_values.values, columns=X_shap.columns)\n",
    "shap_df['expected_value'] = shap_values.base_values\n",
    "shap_df['observation_index'] = X_shap.index\n",
    "\n",
    "\n",
    "print(shap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create Document Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Initialize Retrieval-Augmented Generation (RAG) Components\n",
    "document_store = InMemoryDocumentStore()\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "doc_embedder.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_store_explanations(index, test_df, shap_values, feature_names, explainer_CF, model_wrapper, doc_embedder, document_store, exclude_global_explanations=True):\n",
    "    # Get the test instance by index\n",
    "    instance = test_df.iloc[index]\n",
    "    query = instance[feature_names].to_frame().transpose()  # Convert to DataFrame\n",
    "\n",
    "    # Get the prediction for the original instance using the model wrapper\n",
    "    prediction_probs = model_wrapper.predict_proba(query)\n",
    "    prediction = int(prediction_probs[0, 1] >= 0.5)\n",
    "\n",
    "    # Create the original document\n",
    "    original_doc = Document(\n",
    "        # content=json.dumps(dict(zip(feature_names, instance[feature_names].values)), indent=2),\n",
    "        content=pd.concat([pd.Series({\n",
    "                    \"case_index\": index,\n",
    "                    \"document content type\": \"Original instance\",\n",
    "                    \"scope\": \"local\",\n",
    "                    \"prediction\": 'Malignant' if prediction == 1 else 'Benign'\n",
    "                }),pd.DataFrame([instance[feature_names].values], columns=feature_names).T])[0].to_json(indent=2),\n",
    "        meta={\n",
    "            \"case_index\": index,\n",
    "            \"type\": \"original\",\n",
    "            \"prediction\": 'Malignant' if prediction == 1 else 'Benign'\n",
    "        }\n",
    "    )\n",
    "    docs = [original_doc]    \n",
    "\n",
    "    # Generate SHAP explanations    \n",
    "    shap_values_raw = shap_values.values\n",
    "\n",
    "    # Add local SHAP explanation as a document\n",
    "    shap_values_instance = pd.DataFrame(shap_values_raw, columns=feature_names).iloc[index]\n",
    "    shap_local_doc = Document(\n",
    "                content=pd.concat([pd.Series({\n",
    "                    \"case_index\": index,\n",
    "                    \"document content type\": \"SHAP explanation\",\n",
    "                    \"scope\": \"local\",\n",
    "                    \"prediction\": 'Malignant' if prediction == 1 else 'Benign'\n",
    "                }),shap_values_instance]).to_json(indent=2),\n",
    "                meta={\n",
    "                    \"source\": \"shap_explanation_local\",\n",
    "                    \"case_index\": index,\n",
    "                    \"type\": \"shap\",\n",
    "                    \"scope\": \"local\",\n",
    "                    \"prediction\": 'Malignant' if prediction == 1 else 'Benign'\n",
    "                }\n",
    "            )\n",
    "    docs.append(shap_local_doc)   \n",
    "\n",
    "    # base_values = shap_values[0].base_values\n",
    "\n",
    "    # Calculate predicted classes\n",
    "    # this_y_pred_raw = shap_values_raw.sum(1) + base_values\n",
    "    # this_y_pred_binary = this_y_pred_raw > 0\n",
    "\n",
    "    # Split SHAP values by predicted class\n",
    "    # shap_values_class_0 = shap_values_raw[this_y_pred_binary == 0]\n",
    "    # shap_values_class_1 = shap_values_raw[this_y_pred_binary == 1]\n",
    "\n",
    "    # Function to calculate SHAP metrics\n",
    "    # def calculate_shap_metrics(shap_values_subset):\n",
    "    #     mean_shap = np.mean(shap_values_subset, axis=0)\n",
    "    #     std_shap = np.std(shap_values_subset, axis=0)\n",
    "    #     min_shap = np.min(shap_values_subset, axis=0)\n",
    "    #     max_shap = np.max(shap_values_subset, axis=0)\n",
    "    #     return mean_shap, std_shap, min_shap, max_shap\n",
    "\n",
    "    # Calculate metrics for each class\n",
    "    # mean_shap_0, std_shap_0, min_shap_0, max_shap_0 = calculate_shap_metrics(shap_values_class_0)\n",
    "    # mean_shap_1, std_shap_1, min_shap_1, max_shap_1 = calculate_shap_metrics(shap_values_class_1)\n",
    "\n",
    "    # Correlation between SHAP values and target for each class\n",
    "    # y_test_class_0 = y_test_df[this_y_pred_binary == 0]\n",
    "    # y_test_class_1 = y_test_df[this_y_pred_binary == 1]\n",
    "\n",
    "    # corr_shap_target_0 = [np.corrcoef(shap_values_class_0[:, i], y_test_class_0)[0, 1] for i in range(len(feature_names))]\n",
    "    # corr_shap_target_1 = [np.corrcoef(shap_values_class_1[:, i], y_test_class_1)[0, 1] for i in range(len(feature_names))]\n",
    "\n",
    "    # Store results for class 0\n",
    "    # mean_shap_df_0 = pd.DataFrame(mean_shap_0, index=feature_names, columns=[\"Mean SHAP Value - Class 0\"])\n",
    "    # std_shap_df_0 = pd.DataFrame(std_shap_0, index=feature_names, columns=[\"Std Dev SHAP Value - Class 0\"])\n",
    "    # min_shap_df_0 = pd.DataFrame(min_shap_0, index=feature_names, columns=[\"Min SHAP Value - Class 0\"])\n",
    "    # max_shap_df_0 = pd.DataFrame(max_shap_0, index=feature_names, columns=[\"Max SHAP Value - Class 0\"])\n",
    "    # corr_shap_target_df_0 = pd.DataFrame(corr_shap_target_0, index=feature_names, columns=[\"Correlation with Target - Class 0\"])\n",
    "\n",
    "    # # Store results for class 1\n",
    "    # mean_shap_df_1 = pd.DataFrame(mean_shap_1, index=feature_names, columns=[\"Mean SHAP Value - Class 1\"])\n",
    "    # std_shap_df_1 = pd.DataFrame(std_shap_1, index=feature_names, columns=[\"Std Dev SHAP Value - Class 1\"])\n",
    "    # min_shap_df_1 = pd.DataFrame(min_shap_1, index=feature_names, columns=[\"Min SHAP Value - Class 1\"])\n",
    "    # max_shap_df_1 = pd.DataFrame(max_shap_1, index=feature_names, columns=[\"Max SHAP Value - Class 1\"])\n",
    "    # corr_shap_target_df_1 = pd.DataFrame(corr_shap_target_1, index=feature_names, columns=[\"Correlation with Target - Class 1\"])\n",
    "\n",
    "    # Calculate pairwise correlations between SHAP values for each class\n",
    "    # corr_shap_features_0 = pd.DataFrame(shap_values_class_0, columns=feature_names).corr().stack().reset_index()\n",
    "    # corr_shap_features_1 = pd.DataFrame(shap_values_class_1, columns=feature_names).corr().stack().reset_index()\n",
    "\n",
    "    # if not exclude_global_explanations:\n",
    "    #     # Add global SHAP explanation as a document\n",
    "    #     shap_global_mean_0_doc = Document(\n",
    "    #         content=mean_shap_df_0.to_json(indent=2),\n",
    "    #         meta={\n",
    "    #             \"source\": \"shap_explanation_global\",\n",
    "    #             \"case_index\": index,\n",
    "    #             \"type\": \"shap\",\n",
    "    #             \"scope\": \"global\",\n",
    "    #             \"prediction\": 'Benign'\n",
    "    #         }\n",
    "    #     )\n",
    "    #     shap_global_mean_1_doc = Document(\n",
    "    #         content=mean_shap_df_1.to_json(indent=2),\n",
    "    #         meta={\n",
    "    #             \"source\": \"shap_explanation_global\",\n",
    "    #             \"case_index\": index,\n",
    "    #             \"type\": \"shap\",\n",
    "    #             \"scope\": \"global\",\n",
    "    #             \"prediction\": 'Malignant'\n",
    "    #         }\n",
    "    #     )\n",
    "    #     docs.append(shap_global_mean_0_doc)\n",
    "    #     docs.append(shap_global_mean_1_doc)\n",
    "\n",
    "    #     shap_global_std_0_doc = Document(\n",
    "    #         content=std_shap_df_0.to_json(indent=2),\n",
    "    #         meta={\n",
    "    #             \"source\": \"shap_explanation_global\",\n",
    "    #             \"case_index\": index,\n",
    "    #             \"type\": \"shap\",\n",
    "    #             \"scope\": \"global\",\n",
    "    #             \"prediction\": 'Benign'\n",
    "    #         }\n",
    "    #     )\n",
    "    #     shap_global_std_1_doc = Document(\n",
    "    #         content=std_shap_df_1.to_json(indent=2),\n",
    "    #         meta={\n",
    "    #             \"source\": \"shap_explanation_global\",\n",
    "    #             \"case_index\": index,\n",
    "    #             \"type\": \"shap\",\n",
    "    #             \"scope\": \"global\",\n",
    "    #             \"prediction\": 'Malignant'\n",
    "    #         }\n",
    "    #     )\n",
    "    #     docs.append(shap_global_std_0_doc)\n",
    "    #     docs.append(shap_global_std_1_doc)\n",
    "\n",
    "    #     shap_global_min_0_doc = Document(\n",
    "    #         content=min_shap_df_0.to_json(indent=2),\n",
    "    #         meta={\n",
    "    #             \"source\": \"shap_explanation_global\",\n",
    "    #             \"case_index\": index,\n",
    "    #             \"type\": \"shap\",\n",
    "    #             \"scope\": \"global\",\n",
    "    #             \"prediction\": 'Benign'\n",
    "    #         }\n",
    "    #     )\n",
    "    #     shap_global_min_1_doc = Document(\n",
    "    #         content=min_shap_df_1.to_json(indent=2),\n",
    "    #         meta={\n",
    "    #             \"source\": \"shap_explanation_global\",\n",
    "    #             \"case_index\": index,\n",
    "    #             \"type\": \"shap\",\n",
    "    #             \"scope\": \"global\",\n",
    "    #             \"prediction\": 'Malignant'\n",
    "    #         }\n",
    "    #     )\n",
    "    #     docs.append(shap_global_min_0_doc)\n",
    "    #     docs.append(shap_global_min_1_doc)\n",
    "\n",
    "    #     shap_global_max_0_doc = Document(\n",
    "    #         content=max_shap_df_0.to_json(indent=2),\n",
    "    #         meta={\n",
    "    #             \"source\": \"shap_explanation_global\",\n",
    "    #             \"case_index\": index,\n",
    "    #             \"type\": \"shap\",\n",
    "    #             \"scope\": \"global\",\n",
    "    #             \"prediction\": 'Benign'\n",
    "    #         }\n",
    "    #     )\n",
    "    #     shap_global_max_1_doc = Document(\n",
    "    #         content=max_shap_df_1.to_json(indent=2),\n",
    "    #         meta={\n",
    "    #             \"source\": \"shap_explanation_global\",\n",
    "    #             \"case_index\": index,\n",
    "    #             \"type\": \"shap\",\n",
    "    #             \"scope\": \"global\",\n",
    "    #             \"prediction\": 'Malignant'\n",
    "    #         }\n",
    "    #     )\n",
    "    #     docs.append(shap_global_max_0_doc)\n",
    "    #     docs.append(shap_global_max_1_doc)\n",
    "\n",
    "    #     shap_global_corr_0_doc = Document(\n",
    "    #         content=corr_shap_target_df_0.to_json(indent=2),\n",
    "    #         meta={\n",
    "    #             \"source\": \"shap_explanation_global\",\n",
    "    #             \"case_index\": index,\n",
    "    #             \"type\": \"shap\",\n",
    "    #             \"scope\": \"global\",\n",
    "    #             \"prediction\": 'Benign'\n",
    "    #         }\n",
    "    #     )\n",
    "    #     shap_global_corr_1_doc = Document(\n",
    "    #         content=corr_shap_target_df_1.to_json(indent=2),\n",
    "    #         meta={\n",
    "    #             \"source\": \"shap_explanation_global\",\n",
    "    #             \"case_index\": index,\n",
    "    #             \"type\": \"shap\",\n",
    "    #             \"scope\": \"global\",\n",
    "    #             \"prediction\": 'Malignant'\n",
    "    #         }\n",
    "    #     )\n",
    "    #     docs.append(shap_global_corr_0_doc)\n",
    "    #     docs.append(shap_global_corr_1_doc)\n",
    "    \n",
    "\n",
    "    # Generate counterfactual explanations\n",
    "    explanation = explainer_CF.generate_counterfactuals(\n",
    "        query_instances=query,\n",
    "        total_CFs=3,\n",
    "        desired_class='opposite',\n",
    "        features_to_vary='all'\n",
    "    )\n",
    "\n",
    "    # Access the counterfactuals DataFrame\n",
    "    cf_df = explanation.cf_examples_list[0].final_cfs_df\n",
    "\n",
    "    # Iterate over each counterfactual and create a Document for each\n",
    "    for idx, cf_row in cf_df.iterrows():\n",
    "        cf_values = cf_row[feature_names].values.tolist()\n",
    "\n",
    "        # Get the prediction for the counterfactual instance\n",
    "        cf_query = pd.DataFrame([cf_values], columns=feature_names)\n",
    "        cf_prediction_raw = model_wrapper.predict_proba(cf_query)\n",
    "        cf_prediction = int(cf_prediction_raw[0, 1] >= 0.5)\n",
    "        \n",
    "        # Compute changed features\n",
    "        changed_features = [\n",
    "            feature for feature in feature_names\n",
    "            if not np.isclose(cf_row[feature], instance[feature])\n",
    "        ]\n",
    "        \n",
    "        cf_values_changed = cf_row[changed_features].values.tolist()\n",
    "        cf_query = pd.DataFrame([cf_values_changed], columns=changed_features).T[0]\n",
    "        \n",
    "        # Create a Document for the counterfactual instance\n",
    "        cf_doc = Document(\n",
    "            # content=json.dumps(dict(zip(feature_names, cf_values)), indent=2),\n",
    "            # content=pd.DataFrame(cf_values, columns=feature_names),\n",
    "            # content=pd.concat([pd.Series({\n",
    "            #         \"case_index\": index,\n",
    "            #         \"document content type\": \"Counterfactual Explanation\",\n",
    "            #         \"scope\": \"local\",\n",
    "            #         \"changed features\": changed_features,\n",
    "            #         \"number of changed features\": len(changed_features),\n",
    "            #         \"original prediction\": 'Malignant' if prediction == 1 else 'Benign',\n",
    "            #         \"counterfactual prediction\": 'Malignant' if cf_prediction == 1 else 'Benign'\n",
    "            #     }),cf_query.T])[0].to_json(indent=2),\n",
    "            content=pd.Series({\n",
    "                    \"case_index\": index,\n",
    "                    \"document content type\": \"Counterfactual explanation\",\n",
    "                    \"scope\": \"local\",\n",
    "                    \"changed features\": changed_features,\n",
    "                    \"number of changed features\": len(changed_features),\n",
    "                    \"original prediction\": 'Malignant' if prediction == 1 else 'Benign',\n",
    "                    \"counterfactual prediction\": 'Malignant' if cf_prediction == 1 else 'Benign',\n",
    "                    \"feature values changed to\": cf_query\n",
    "                }).to_json(indent=2),\n",
    "            meta={\n",
    "                \"source\": f\"counterfactual_instance\",\n",
    "                \"case_index\": index,\n",
    "                \"type\": \"counterfactual\",\n",
    "                \"scope\": \"local\",\n",
    "                \"changed features\": changed_features,\n",
    "                \"number of changed features\": len(changed_features),\n",
    "                \"original prediction\": 'Malignant' if prediction == 1 else 'Benign',\n",
    "                \"counterfactual prediction\": 'Malignant' if cf_prediction == 1 else 'Benign'\n",
    "            }\n",
    "        )\n",
    "        docs.append(cf_doc)\n",
    "\n",
    "    # Embed the documents\n",
    "    docs_with_embeddings = doc_embedder.run(docs)\n",
    "\n",
    "    # Write the documents to the document store\n",
    "    document_store.write_documents(docs_with_embeddings[\"documents\"], policy=DuplicatePolicy.SKIP)\n",
    "    # document_store.write_documents(documents=docs, policy=DuplicatePolicy.SKIP)\n",
    "\n",
    "# Example usage:\n",
    "# generate_and_store_explanations(index, test_df, shap_local, shap_global, feature_names, explainer_CF, model_wrapper, doc_embedder, document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_store_explanations(index=1, test_df=test_df, shap_values=shap_values, feature_names=feature_names, explainer_CF=explainer_DiCE, model_wrapper=model_wrapper, doc_embedder=doc_embedder, document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_filters = {\n",
    "    \"operator\": \"AND\",\n",
    "    \"conditions\": [\n",
    "        {\"field\": \"source\", \"operator\": \"==\", \"value\": \"shap_explanation_local\"}        \n",
    "    ],\n",
    "}\n",
    "\n",
    "# print(document_store.filter_documents()[0].content)\n",
    "# print(document_store.filter_documents()[1].content)\n",
    "# print(document_store.filter_documents()[2].content)\n",
    "# print(document_store.filter_documents()[3].content)\n",
    "# print(document_store.filter_documents())\n",
    "print(document_store.filter_documents(filters=this_filters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create RAG Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable logging\n",
    "# logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "# logging.getLogger(\"haystack\").setLevel(logging.DEBUG)\n",
    "\n",
    "# tracing.tracer.is_content_tracing_enabled = True\n",
    "# tracing.enable_tracing(LoggingTracer(tags_color_strings={\"haystack.component.input\": \"\\x1b[1;31m\", \"haystack.component.name\": \"\\x1b[1;34m\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM \n",
    "# generator = HuggingFaceLocalChatGenerator(\n",
    "generator = HuggingFaceLocalGenerator(\n",
    "    huggingface_pipeline_kwargs={\"device\":0, \"torch_dtype\":torch.bfloat16},\n",
    "    generation_kwargs={\"max_new_tokens\": 1024, \"max_time\":120},\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    # model=\"meta-llama/Llama-3.2-3B\"\n",
    "    # model=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    # model=\"HuggingFaceH4/zephyr-7b-beta\"\n",
    "    )\n",
    "generator.warm_up()\n",
    "\n",
    "# Create memory store for chat messages\n",
    "memory_store = InMemoryChatMessageStore()\n",
    "\n",
    "rag_pipe_main = Pipeline()\n",
    "\n",
    "## Main RAG\n",
    "# components for RAG\n",
    "rag_pipe_main.add_component(\"text_embedder_main\", SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "rag_pipe_main.add_component(\"retriever_main\", InMemoryEmbeddingRetriever(document_store, top_k=5))\n",
    "rag_pipe_main.add_component(\"prompt_builder_main\", ChatPromptBuilder(variables=[\"query\", \"documents\", \"memories\"], required_variables=[\"query\", \"documents\", \"memories\"]))\n",
    "\n",
    "# components for memory\n",
    "rag_pipe_main.add_component(\"memory_retriever_main\", ChatMessageRetriever(memory_store))\n",
    "rag_pipe_main.add_component(\"memory_joiner_main\", BranchJoiner(List[ChatMessage]))\n",
    "rag_pipe_main.add_component(\"memory_writer_main\", ChatMessageWriter(memory_store))\n",
    "\n",
    "\n",
    "# connections for RAG\n",
    "rag_pipe_main.connect(\"text_embedder_main\", \"retriever_main\")\n",
    "rag_pipe_main.connect(\"retriever_main.documents\", \"prompt_builder_main.documents\")\n",
    "\n",
    "# connections for memory\n",
    "rag_pipe_main.connect(\"memory_joiner_main\", \"memory_writer_main\")\n",
    "rag_pipe_main.connect(\"memory_retriever_main\", \"prompt_builder_main.memories\")\n",
    "\n",
    "## Chat memory module\n",
    "rag_pipe_memory_writer = Pipeline()\n",
    "\n",
    "# components for memory\n",
    "rag_pipe_memory_writer.add_component(\"memory_joiner\", BranchJoiner(List[ChatMessage]))\n",
    "rag_pipe_memory_writer.add_component(\"memory_writer\", ChatMessageWriter(memory_store))\n",
    "\n",
    "# connections for memory\n",
    "rag_pipe_memory_writer.connect(\"memory_joiner\", \"memory_writer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipe_main.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipe_memory_writer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = ChatMessage.from_system(\"\"\"You are a helpful AI assistant using provided supporting documents and conversation history to assist humans.\"\"\")\n",
    "\n",
    "user_message_template =\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>Given the conversation history and the provided supporting documents, fulfill the task or give an answer to the question.\n",
    "Note that supporting documents are not part of the conversation. If questions can't be answered or tasks cannot be fulfilled by using supporting documents, state this in your response.\n",
    "\n",
    "The following are criteria for counterfactual explanations:\n",
    "Validity: The counterfactual must accurately reflect changes that would lead to a different diagnostic prediction by the model.\n",
    "Proximity: Changes should be minimal, keeping the counterfactual instance close to the original case.\n",
    "Sparsity: The explanation should involve altering as few features as possible.\n",
    "Feasibility: Proposed changes must be medically plausible, even if not actionable.\n",
    "\n",
    "SHAP explanations provide information regarding the contribution of each feature on the prediction.\n",
    "\n",
    "Supporting documents contain information regarding the conducted analysis of human breast tissue with a suspision for breast cancer along with a prediction from a specialized machine learning model.\n",
    "Evaluate all counterfactual explanations yielded by the machine learning model within the provided documents using criteria for counterfactual explanations focusing on the changed features and feature values.\n",
    "By default, focus your response on analysis regarding feature impact using SHAP explanations and connect your observations to your evaluation of counterfactual explanations.\n",
    "\n",
    "    Conversation history:\n",
    "    {% for memory in memories %}\n",
    "        {{ memory.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    Supporting documents:\n",
    "    {% for doc in documents %}\n",
    "        {{ doc.content }}\n",
    "    {% endfor %}\n",
    "    \n",
    "    Documents with the document content type \"Original instance\" contain the original feature values for each feature variable.\n",
    "    Documents with the document content type \"Counterfactual explanation\" contain changed feature values for each feature variable.\n",
    "    Documents with the document content type \"SHAP explanation\" contain SHAP values for each feature variable instead of feature values. These SHAP values show the impact of the each feature and its value on the model prediction for the original instance.\n",
    "\n",
    "    \\\\Task or Question: {{query}}\n",
    "    \\\\Response:<|eot_id|>\n",
    "  <|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "user_message = ChatMessage.from_user(user_message_template)\n",
    "this_chat_template = [system_message, user_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_demo(this_index):\n",
    "    # this_index = 1\n",
    "    document_store = InMemoryDocumentStore()\n",
    "    generate_and_store_explanations(index=this_index, test_df=test_df, shap_values=shap_values, feature_names=feature_names, explainer_CF=explainer_DiCE, model_wrapper=model_wrapper, doc_embedder=doc_embedder, document_store=document_store)\n",
    "    doc_filter = {\n",
    "        \"operator\": \"AND\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"case_index\", \"operator\": \"==\", \"value\": this_index} # only retrieve Documents for the relevant data instance        \n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def this_chatbot(message, history):\n",
    "        res = rag_pipe_main.run(\n",
    "                data={\n",
    "                    \"retriever_main\": {\"filters\": doc_filter},\n",
    "                    \"text_embedder_main\": {\"text\": message},\n",
    "                    \"prompt_builder_main\": {\"template\": this_chat_template, \"query\": message},\n",
    "                    \"memory_joiner_main\": {\"value\": [ChatMessage.from_user(message)] },\n",
    "                    \"memory_retriever_main\": {\"last_k\": 5}            \n",
    "                }\n",
    "            )\n",
    "        \n",
    "        prompt_for_generator = res[\"prompt_builder_main\"][\"prompt\"][1].content\n",
    "        gen_res = generator.run(prompt_for_generator)\n",
    "        rag_pipe_memory_writer.run(\n",
    "                data={\n",
    "                    \"memory_joiner\": {\"value\": [ChatMessage.from_assistant(gen_res[\"replies\"][0])] }\n",
    "                }\n",
    "\n",
    "            ) \n",
    "        \n",
    "        return gen_res[\"replies\"][0]\n",
    "    \n",
    "    demo = gr.ChatInterface(\n",
    "        chatbot=gr.Chatbot(height=800, placeholder=\"<strong>LLM-assisted exploration of local XAI analysis. </strong><br>Ask Me Anything\"),\n",
    "        fn=this_chatbot,\n",
    "        # inputs=[\"text\", gr.Slider(0, 100)],\n",
    "        examples=[\n",
    "            \"Which of the counterfactual explanations fits the best in order to explain the model prediction?\",\n",
    "            \"Explain the model prediction using only local counterfactual explanations.\",\n",
    "            \"Explain the model prediction using only local SHAP explanations.\",\n",
    "        ],\n",
    "        title=\"LLM powered XAI\",\n",
    "        description=\"Explore counterfactual explanation and SHAP explanations for individual test instances.\",\n",
    "        theme=\"soft\",\n",
    "    )   \n",
    "\n",
    "    demo.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_demo(this_index= 1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
